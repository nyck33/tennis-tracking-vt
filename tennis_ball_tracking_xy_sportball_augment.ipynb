{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76b30745-9dcf-4ee8-acdc-c562a015919b",
   "metadata": {
    "id": "76b30745-9dcf-4ee8-acdc-c562a015919b",
    "noteable": {
     "cell_type": "markdown"
    },
    "tags": [
     "show_line_numbers"
    ]
   },
   "source": [
    "# Tennis Ball Tracking with Vision Transformer\n",
    "\n",
    "In this notebook, we will outline the steps to fine-tune a Vision Transformer (ViT) model for the task of tracking a tennis ball in video frames. The goal is to predict the x, y coordinates of the tennis ball and the event type (flying, bouncing, being hit) in each frame.\n",
    "\n",
    "The dataset we will use is provided by TrackNet and contains broadcast TV tennis match videos along with accompanying .csv files that annotate the x, y location of the tennis ball and the event type in each frame.\n",
    "\n",
    "The steps we will follow are:\n",
    "\n",
    "1. **Data Preparation:** Extract frames from the videos and save them as individual images. The labels for each image (the x, y coordinates of the tennis ball and the event type) will be extracted from the accompanying .csv file.\n",
    "\n",
    "2. **Data Preprocessing:** Preprocess the images to be in the format expected by the Vision Transformer model. This typically involves resizing the images to the expected input size of the model (224x224 for the base Vision Transformer model), and normalizing the pixel values.\n",
    "\n",
    "3. **Model Preparation:** Load the pre-trained Vision Transformer model, and modify its final layer to match the number of output classes for our tasks. For the x, y coordinate prediction task, we will add a fully connected layer with 2 output units (for the x and y coordinates). For the event type prediction task, we will add a fully connected layer with 3 output units (for the 3 event types), followed by a softmax activation function.\n",
    "\n",
    "4. **Training Loop:** Define a training loop where we feed the preprocessed images to the model, compute the loss for both tasks (using a suitable loss function for each task), and update the model's weights based on the total loss. The total loss will be a weighted sum of the two individual losses, where the weights reflect the importance of each task.\n",
    "\n",
    "5. **Evaluation:** After training the model for a certain number of epochs, we will evaluate its performance on a validation set. We will compute the loss and accuracy for each task, and adjust the model's hyperparameters or the training process as needed to improve its performance.\n",
    "\n",
    "6. **Inference:** Once we are satisfied with the model's performance, we can use it to predict the x, y coordinates and event type of the tennis ball in new video frames.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da08527a-5cd2-4d12-a22f-cd8641bada8f",
   "metadata": {
    "id": "da08527a-5cd2-4d12-a22f-cd8641bada8f",
    "noteable": {
     "cell_type": "markdown"
    },
    "tags": [
     "show_line_numbers"
    ]
   },
   "source": [
    "## Step 1: Data Preparation\n",
    "\n",
    "In this step, we will extract the frames from the videos and save them as individual images. The labels for each image (the x, y coordinates of the tennis ball and the event type) will be extracted from the accompanying .csv file.\n",
    "\n",
    "We will use the OpenCV library to read the video files and extract the frames. The pandas library will be used to read the .csv file and extract the labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d477351e-3d49-49a1-ae26-96b9e3a3a084",
   "metadata": {},
   "source": [
    "### ImageNet normalization used\n",
    "However, when using pre-trained models like the Vision Transformer google/vit-base-patch16-224, it's important to match the preprocessing steps that were applied to the data during the model's original training. In this case, the model was trained on the ImageNet dataset, which was normalized using the specific mean and standard deviation values for the RGB channels that I mentioned earlier.\n",
    "\n",
    "So, while normalizing to the range {0,1} is not wrong per se, it might not yield the best results when using this specific pre-trained model. The model might perform better if the input images are normalized in the same way the training data was normalized.\n",
    "\n",
    "Therefore, I would recommend adjusting your normalization step to match the ImageNet normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107418fc-c51f-42a4-8769-6435bc99b5bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mEIO: i/o error, scandir '/mnt/d/tennis/tracknet'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd6fb79-001f-4cd0-806c-4f80fa523831",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dcd6fb79-001f-4cd0-806c-4f80fa523831",
    "noteable": {
     "cell_type": "code"
    },
    "outputId": "b1d321e1-7169-46dc-9e32-0cbd656fa80e",
    "tags": [
     "show_line_numbers"
    ]
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mEIO: i/o error, scandir '/mnt/d/tennis/tracknet'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the size to resize the images to\n",
    "image_size = (224, 224)\n",
    "\n",
    "# Define the ImageNet mean and standard deviation\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "# Define the path to the dataset directory\n",
    "dataset_dir = Path('Dataset/Dataset')\n",
    "\n",
    "def process_image(image_file):\n",
    "    try:\n",
    "        # Read the image\n",
    "        image = cv2.imread(str(image_file))\n",
    "\n",
    "        # Resize the image\n",
    "        image = cv2.resize(image, image_size)\n",
    "\n",
    "        # Convert the image to RGB and normalize the pixel values\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) / 255.0\n",
    "        image = (image - mean) / std\n",
    "\n",
    "        # Save the preprocessed image\n",
    "        np.save(image_file.with_suffix('.npy'), image)\n",
    "    except Exception as e:\n",
    "        print(f'Error processing image {image_file}: {e}')\n",
    "\n",
    "def process_labels(csv_file):\n",
    "    try:\n",
    "        labels_df = pd.read_csv(csv_file)\n",
    "\n",
    "        # Adjust the x, y coordinates to match the new image size\n",
    "        original_image_size = cv2.imread(str(next(csv_file.parent.glob('*.jpg')))).shape[:2][::-1]\n",
    "        labels_df['x-coordinate'] *= image_size[0] / original_image_size[0]\n",
    "        labels_df['y-coordinate'] *= image_size[1] / original_image_size[1]\n",
    "        \n",
    "        # normalize the x, y coordinates\n",
    "        labels_df['x-coordinate'] /= image_size[0]\n",
    "        labels_df['y-coordinate'] /= image_size[1]\n",
    "\n",
    "        # Save the preprocessed labels for each image individually \n",
    "        for index, row in labels_df.iterrows():\n",
    "            label_data = row[['x-coordinate', 'y-coordinate']].to_numpy()\n",
    "            np.save(csv_file.parent / f'{row[\"file name\"]}_labels.npy', label_data.astype(np.float32))\n",
    "\n",
    "        # Save the updated dataframe\n",
    "        labels_df.to_csv(csv_file.with_name(f'{csv_file.stem}_updated.csv'), index=False)\n",
    "    except Exception as e:\n",
    "        print(f'Error processing labels {csv_file}: {e}')\n",
    "\n",
    "prep_data = False\n",
    "# Loop over the game directories\n",
    "if prep_data:\n",
    "    for game_dir in tqdm(list(dataset_dir.glob('game*')), desc='Processing games'):\n",
    "        # Loop over the clip directories in each game directory\n",
    "        for clip_dir in game_dir.glob('Clip*'):\n",
    "            # Loop over the image files in each clip directory\n",
    "            for image_file in clip_dir.glob('*.jpg'):\n",
    "                process_image(image_file)\n",
    "\n",
    "            # Read the .csv file in each clip directory\n",
    "            csv_files = list(clip_dir.glob('Label.csv'))\n",
    "            if csv_files:\n",
    "                process_labels(csv_files[0])\n",
    "\n",
    "    # Now, we have the preprocessed images and labels.\n",
    "    # We can proceed to the next step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5ca757",
   "metadata": {
    "id": "fc5ca757"
   },
   "source": [
    "## Step 2: Create PyTorch Datasets and DataLoaders\n",
    "\n",
    "In this step, we will create PyTorch `Dataset` objects for the training and validation sets. A `Dataset` is a PyTorch abstraction that allows us to encapsulate our data and provide a way to access it. We will also create `DataLoader` objects, which allow us to load data in batches during training, shuffle the data, and parallelize the data loading process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1408192e",
   "metadata": {
    "id": "1408192e"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mEIO: i/o error, scandir '/mnt/d/tennis/tracknet'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import os\n",
    "import glob\n",
    "from torch import tensor\n",
    "\n",
    "class TennisDataset(Dataset):\n",
    "    def __init__(self, dataframe, root_dir, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "    \n",
    "        row = self.dataframe.iloc[idx]\n",
    "    \n",
    "        # Extract the base file name from the file name and the directories\n",
    "        base_file_name = row[\"file name\"].split('.jpg_labels')[0]\n",
    "        clip_directory = row[\"clip directory\"]\n",
    "        game_directory = row[\"game directory\"]\n",
    "    \n",
    "        # Construct the path to the image and labels\n",
    "        image_path = self.root_dir / game_directory / clip_directory / f'{base_file_name}.npy'\n",
    "        labels_path = self.root_dir / game_directory / clip_directory / f'{base_file_name}.jpg_labels.npy'\n",
    "    \n",
    "        # Rest of the code...\n",
    "        image = np.load(image_path, allow_pickle=True)\n",
    "        labels = np.load(labels_path, allow_pickle=True)\n",
    "\n",
    "\n",
    "        # Convert the labels to a PyTorch tensor\n",
    "        labels = tensor(labels, dtype=torch.float32)\n",
    "    \n",
    "        # Print the shape of the image\n",
    "        #print(f\"Original shape: {image.shape}\")\n",
    "    \n",
    "        # Transpose the image dimensions\n",
    "        image = image.transpose((2, 0, 1))\n",
    "    \n",
    "        # Print the new shape of the image\n",
    "        #print(f\"Transposed shape: {image.shape}\")\n",
    "\n",
    "        # Convert the image and labels to PyTorch tensors\n",
    "        image = tensor(image, dtype=torch.float32)\n",
    "\n",
    "        \n",
    "    \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "    \n",
    "        return image, labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26f4b88-178e-4864-b22d-f3de1d18c823",
   "metadata": {},
   "source": [
    "## Step 3: Split the Dataset into Training and Validation Sets\n",
    "\n",
    "In this step, we will split the dataset into training and validation sets. The training set is used to train the model, while the validation set is used to evaluate the model's performance during training. This helps us to monitor the model for overfitting, which occurs when the model performs well on the training data but poorly on new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5871b9-795a-4080-b905-05f9b19762e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mEIO: i/o error, scandir '/mnt/d/tennis/tracknet'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Define the path to the dataset directory\n",
    "dataset_dir = Path('Dataset/Dataset')\n",
    "\n",
    "# Create a list to store the file names and their parent directories\n",
    "data = []\n",
    "\n",
    "# Loop over the game directories\n",
    "for game_dir in dataset_dir.glob('game*'):\n",
    "    # Loop over the clip directories in each game directory\n",
    "    for clip_dir in game_dir.glob('Clip*'):\n",
    "        # Loop over the image files in each clip directory\n",
    "        for image_file in clip_dir.glob('*.npy'):\n",
    "            # Append the base file name and its parent directories to the list\n",
    "            data.append({'file name': os.path.splitext(image_file.name)[0], 'clip directory': clip_dir.name, 'game directory': game_dir.name})\n",
    "\n",
    "# Create a DataFrame from the list\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Split the DataFrame into training and validation sets\n",
    "train_df, val_df = train_test_split(df, test_size=0.2)\n",
    "\n",
    "# Now you can use train_df and val_df to create your datasets\n",
    "train_dataset = TennisDataset(train_df, root_dir=dataset_dir)\n",
    "val_dataset = TennisDataset(val_df, root_dir=dataset_dir)\n",
    "\n",
    "# And create your DataLoaders\n",
    "# nproc shows 12\n",
    "# batch size 32 is too big locally so try 16, 8, 4, 2\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=12)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, num_workers=12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309d3078-026b-4b5b-8bcd-934db86b9c9f",
   "metadata": {},
   "source": [
    "### check one of the image .npy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54626148-bec9-4dee-8fa3-518512391e58",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mEIO: i/o error, scandir '/mnt/d/tennis/tracknet'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31b94a4-b3b8-4a20-8abe-7abedec8eff1",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mEIO: i/o error, scandir '/mnt/d/tennis/tracknet'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "test_path = \"Dataset/Dataset/game1/Clip1/0000.npy\"\n",
    "data = np.load(test_path, allow_pickle=True)\n",
    "print(data.shape, data.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55290b7e",
   "metadata": {
    "id": "55290b7e"
   },
   "source": [
    "### Step 3B, data augmentation (optional)\n",
    "In this code, we define a transforms.Compose object that first converts the image to a PIL Image, then applies a random horizontal flip with a probability of 0.5, and finally converts the image back to a tensor. We then modify the TennisDataset class to accept an optional transform argument and apply this transform to the images in the __getitem__ method. If an image is flipped, we also flip the x-coordinate of the ball.\n",
    "\n",
    "Please note that this is a simple example and might not work perfectly for your specific use case. For example, the RandomHorizontalFlip transform uses a fixed random state, so the same images will always be flipped. If you want truly random flipping, you might need to implement your own flipping transform. Also, this code assumes that the 'x' coordinate is the first element in the label array, and that it is a value between 0 and 1 representing the relative position of the ball in the frame. If your data is different, you would need to adjust the code accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c05fba",
   "metadata": {
    "id": "13c05fba"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mEIO: i/o error, scandir '/mnt/d/tennis/tracknet'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "'''\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define the data augmentation\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "class TennisDataset(Dataset):\n",
    "    def __init__(self, df, frames_dir, transform=None):\n",
    "        self.df = df\n",
    "        self.frames_dir = frames_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image = np.load(os.path.join(self.frames_dir, f'{row.clip_number}_{row.frame_number}.jpg.npy'))\n",
    "        label = row[['x', 'y', 'event_type']].values\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            # If the image was flipped, flip the x-coordinate of the ball\n",
    "            if self.transform.transforms[1].p == 1:\n",
    "                label[0] = 1 - label[0]\n",
    "\n",
    "        return torch.from_numpy(image), torch.from_numpy(label)\n",
    "\n",
    "# Create the datasets with data augmentation\n",
    "#train_dataset = TennisDataset(train_df, frames_dir, transform=data_transforms)\n",
    "#al_dataset = TennisDataset(val_df, frames_dir)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936ab9c4-a521-4b47-982e-56798dfff6bb",
   "metadata": {
    "id": "936ab9c4-a521-4b47-982e-56798dfff6bb",
    "noteable": {
     "cell_type": "markdown"
    },
    "tags": [
     "show_line_numbers"
    ]
   },
   "source": [
    "## Step 4: Model Preparation\n",
    "\n",
    "In this step, we will load the pre-trained Vision Transformer model, and modify its final layer to match the number of output classes for our tasks. For the x, y coordinate prediction task, we will add a fully connected layer with 2 output units (for the x and y coordinates). For the event type prediction task, we will add a fully connected layer with 3 output units (for the 3 event types), followed by a softmax activation function.\n",
    "\n",
    "We will use the Hugging Face Transformers library to load the pre-trained Vision Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d53dcf6-e6b3-42b9-ace9-5857175f437b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "360c6656a1764c6ca29b87e816ffab88",
      "a9a7637e309f47629666e6c26d604ead",
      "a0f2458163da4d5e8a0731a2aaa3692b",
      "b2aec71425b14cef82277e72b711d8c5",
      "b62a221f14724aa89b581709f409cc18",
      "41726533a5b14b50aa0195b3880acff7",
      "ae0c08319ef142ab975c8c6e7eeaf78c",
      "b7654161092d4854b7127613a413a178",
      "4b2654ba348c435cad6b4dca149e5494",
      "a2313b72c7a64a2d861dcbd7eb1a998c",
      "0692e30fe97d4cc198cae4e937e96eaf"
     ]
    },
    "id": "1d53dcf6-e6b3-42b9-ace9-5857175f437b",
    "noteable": {
     "cell_type": "code"
    },
    "outputId": "96af5039-3642-4b67-91df-b62b8aaf0c84",
    "tags": [
     "show_line_numbers"
    ]
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mEIO: i/o error, scandir '/mnt/d/tennis/tracknet'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import ViTModel, ViTConfig\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load the pre-trained Vision Transformer model\n",
    "config = ViTConfig.from_pretrained('google/vit-base-patch16-224')\n",
    "model = ViTModel(config)\n",
    "\n",
    "# Modify the final layer\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Linear(config.hidden_size, 2),  # For the x, y coordinate prediction task\n",
    "\n",
    ")\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Now, we have the modified Vision Transformer model.\n",
    "# We can proceed to the next step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992e2635",
   "metadata": {
    "id": "992e2635"
   },
   "source": [
    "## Step 5: Define the Loss Function and Optimizer\n",
    "\n",
    "In this step, we will define the loss function and the optimizer. The loss function measures how well the model's predictions match the actual values. The optimizer is used to update the model's parameters based on the gradients of the loss function with respect to the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cebfc1",
   "metadata": {
    "id": "13cebfc1"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mEIO: i/o error, scandir '/mnt/d/tennis/tracknet'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Now, we have the loss function and optimizer.\n",
    "# We can proceed to the next step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e30f114-d548-474b-8b9e-e1b4eab557ea",
   "metadata": {
    "id": "1e30f114-d548-474b-8b9e-e1b4eab557ea",
    "noteable": {
     "cell_type": "markdown"
    },
    "tags": [
     "show_line_numbers"
    ]
   },
   "source": [
    "## Step 5: Training Loop\n",
    "\n",
    "In this step, we will define a training loop where we feed the preprocessed images to the model, compute the loss for both tasks (using a suitable loss function for each task), and update the model's weights based on the total loss. The total loss will be a weighted sum of the two individual losses, where the weights reflect the importance of each task.\n",
    "\n",
    "We will use the PyTorch library to define the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eec039a",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mEIO: i/o error, scandir '/mnt/d/tennis/tracknet'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter('runs/tennis_ball_tracking_xy_sportball')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b6aea9-bdf9-428c-acaa-61d7e00c57a8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 703
    },
    "id": "c5b6aea9-bdf9-428c-acaa-61d7e00c57a8",
    "noteable": {
     "cell_type": "code"
    },
    "outputId": "a9209614-4665-437b-fe39-f493b3af5fe3",
    "tags": [
     "show_line_numbers"
    ]
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mEIO: i/o error, scandir '/mnt/d/tennis/tracknet'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# save weights every epoch\n",
    "# Define the directory name\n",
    "dir_name = \"saved_models\"\n",
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "# Create the full directory path\n",
    "full_dir_path = os.path.join(cwd, dir_name)\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(full_dir_path, exist_ok=True)\n",
    "\n",
    "# Define the number of epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# Loop over the epochs\n",
    "for epoch in tqdm(range(num_epochs), desc='Epochs'):\n",
    "    # Train\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, labels in tqdm(train_dataloader, desc='Training', leave=False):\n",
    "        # Move the images and labels to the GPU if available\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Process the labels\n",
    "        #labels = process_labels(labels)\n",
    "        if labels is None:\n",
    "            print(f'error: labels is {labels}')\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        if outputs is None:\n",
    "            print(f'error: output is {outputs} for {labels}')\n",
    "        else:\n",
    "            print(outputs)\n",
    "        loss = criterion(outputs.last_hidden_state, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        \n",
    "        writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "        \n",
    "\n",
    "    train_loss /= len(train_dataloader.dataset)\n",
    "\n",
    "    # Validate\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_dataloader, desc='Validation', leave=False):\n",
    "            # Move the images and labels to the GPU if available\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Process the labels\n",
    "            labels = process_labels(labels)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs.last_hidden_state, labels)\n",
    "\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            \n",
    "            writer.add_scalar('Loss/val', val_loss, epoch)\n",
    "\n",
    "    val_loss /= len(val_dataloader.dataset)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "    torch.save(model.state_dict(), f'{full_dir_path}/model_weights_epoch-{epoch+1}.pth')\n",
    "#close SummaryWriter\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a55873",
   "metadata": {},
   "source": [
    "# TensorBoard\n",
    "```bash\n",
    "tensorboard --logdir=runs \n",
    "```\n",
    "in the command line, and view the results in your web browser at localhost:6006."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02f1027-1676-4771-b9a7-d5942e5dba2d",
   "metadata": {
    "id": "f02f1027-1676-4771-b9a7-d5942e5dba2d",
    "noteable": {
     "cell_type": "markdown"
    },
    "tags": [
     "show_line_numbers"
    ]
   },
   "source": [
    "## Step 6: Evaluation\n",
    "\n",
    "After training the model for a certain number of epochs, we will evaluate its performance on a validation set. We will compute the loss and accuracy for each task, and adjust the model's hyperparameters or the training process as needed to improve its performance.\n",
    "\n",
    "We will use the PyTorch library to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3bca20-4dc5-416b-a85a-b1406c16bcc7",
   "metadata": {
    "id": "db3bca20-4dc5-416b-a85a-b1406c16bcc7",
    "noteable": {
     "cell_type": "code"
    },
    "tags": [
     "show_line_numbers"
    ]
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mEIO: i/o error, scandir '/mnt/d/tennis/tracknet'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Define the DataLoader for the validation data\n",
    "val_loader = val_dataloader  # This is the DataLoader object containing your validation data\n",
    "\n",
    "# Evaluation loop\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # Do not compute gradients\n",
    "    total_loss = 0\n",
    "    for images, labels in val_loader:\n",
    "        # Move the data to the GPU if available\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Compute the average loss\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "\n",
    "    print(f'Validation Loss: {avg_loss}')\n",
    "\n",
    "# Now, we have evaluated the model.\n",
    "# We can proceed to the next step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df708e4a-762c-4e6f-b25c-ee8aeb2f3d5e",
   "metadata": {
    "id": "df708e4a-762c-4e6f-b25c-ee8aeb2f3d5e",
    "noteable": {
     "cell_type": "markdown"
    },
    "tags": [
     "show_line_numbers"
    ]
   },
   "source": [
    "## Step 7: Inference\n",
    "\n",
    "Once we are satisfied with the model's performance, we can use it to predict the x, y coordinates and event type of the tennis ball in new video frames.\n",
    "\n",
    "We will use the PyTorch library to perform inference with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2555ec-c78d-4719-8c3c-76b8a920b9a1",
   "metadata": {
    "id": "5c2555ec-c78d-4719-8c3c-76b8a920b9a1",
    "noteable": {
     "cell_type": "code"
    },
    "tags": [
     "show_line_numbers"
    ]
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mEIO: i/o error, scandir '/mnt/d/tennis/tracknet'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Define the DataLoader for the test data\n",
    "test_data = ...  # This should be a PyTorch Dataset object containing your test data\n",
    "test_loader = DataLoader(test_data, batch_size=32)\n",
    "\n",
    "# Inference loop\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # Do not compute gradients\n",
    "    for images in test_loader:\n",
    "        # Move the data to the GPU if available\n",
    "        images = images.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Compute the predictions\n",
    "        predictions = outputs.argmax(dim=1)\n",
    "\n",
    "        # Here, you can do whatever you want with the predictions.\n",
    "        # For example, you can visualize the predictions on the images.\n",
    "\n",
    "# Now, we have performed inference with the model.\n",
    "# This is the end of the process.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "display_mode": "default",
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "noteable": {
   "last_delta_id": "974eaaad-d75a-465f-b260-2a979e36c0b5",
   "last_transaction_id": "50aa7114-aae6-4a77-b18f-eb8960fa539a"
  },
  "nteract": {
   "version": "noteable@2.9.0"
  },
  "selected_hardware_size": "small",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0692e30fe97d4cc198cae4e937e96eaf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "360c6656a1764c6ca29b87e816ffab88": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a9a7637e309f47629666e6c26d604ead",
       "IPY_MODEL_a0f2458163da4d5e8a0731a2aaa3692b",
       "IPY_MODEL_b2aec71425b14cef82277e72b711d8c5"
      ],
      "layout": "IPY_MODEL_b62a221f14724aa89b581709f409cc18"
     }
    },
    "41726533a5b14b50aa0195b3880acff7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b2654ba348c435cad6b4dca149e5494": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a0f2458163da4d5e8a0731a2aaa3692b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b7654161092d4854b7127613a413a178",
      "max": 69665,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4b2654ba348c435cad6b4dca149e5494",
      "value": 69665
     }
    },
    "a2313b72c7a64a2d861dcbd7eb1a998c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a9a7637e309f47629666e6c26d604ead": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_41726533a5b14b50aa0195b3880acff7",
      "placeholder": "​",
      "style": "IPY_MODEL_ae0c08319ef142ab975c8c6e7eeaf78c",
      "value": "Downloading (…)lve/main/config.json: 100%"
     }
    },
    "ae0c08319ef142ab975c8c6e7eeaf78c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b2aec71425b14cef82277e72b711d8c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a2313b72c7a64a2d861dcbd7eb1a998c",
      "placeholder": "​",
      "style": "IPY_MODEL_0692e30fe97d4cc198cae4e937e96eaf",
      "value": " 69.7k/69.7k [00:00&lt;00:00, 1.84MB/s]"
     }
    },
    "b62a221f14724aa89b581709f409cc18": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b7654161092d4854b7127613a413a178": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
