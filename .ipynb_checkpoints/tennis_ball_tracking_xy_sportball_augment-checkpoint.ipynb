{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76b30745-9dcf-4ee8-acdc-c562a015919b",
   "metadata": {
    "id": "76b30745-9dcf-4ee8-acdc-c562a015919b",
    "noteable": {
     "cell_type": "markdown"
    },
    "tags": [
     "show_line_numbers"
    ]
   },
   "source": [
    "# Tennis Ball Tracking with Vision Transformer\n",
    "\n",
    "In this notebook, we will outline the steps to fine-tune a Vision Transformer (ViT) model for the task of tracking a tennis ball in video frames. The goal is to predict the x, y coordinates of the tennis ball and the event type (flying, bouncing, being hit) in each frame.\n",
    "\n",
    "The dataset we will use is provided by TrackNet and contains broadcast TV tennis match videos along with accompanying .csv files that annotate the x, y location of the tennis ball and the event type in each frame.\n",
    "\n",
    "The steps we will follow are:\n",
    "\n",
    "1. **Data Preparation:** Extract frames from the videos and save them as individual images. The labels for each image (the x, y coordinates of the tennis ball and the event type) will be extracted from the accompanying .csv file.\n",
    "\n",
    "2. **Data Preprocessing:** Preprocess the images to be in the format expected by the Vision Transformer model. This typically involves resizing the images to the expected input size of the model (224x224 for the base Vision Transformer model), and normalizing the pixel values.\n",
    "\n",
    "3. **Model Preparation:** Load the pre-trained Vision Transformer model, and modify its final layer to match the number of output classes for our tasks. For the x, y coordinate prediction task, we will add a fully connected layer with 2 output units (for the x and y coordinates). For the event type prediction task, we will add a fully connected layer with 3 output units (for the 3 event types), followed by a softmax activation function.\n",
    "\n",
    "4. **Training Loop:** Define a training loop where we feed the preprocessed images to the model, compute the loss for both tasks (using a suitable loss function for each task), and update the model's weights based on the total loss. The total loss will be a weighted sum of the two individual losses, where the weights reflect the importance of each task.\n",
    "\n",
    "5. **Evaluation:** After training the model for a certain number of epochs, we will evaluate its performance on a validation set. We will compute the loss and accuracy for each task, and adjust the model's hyperparameters or the training process as needed to improve its performance.\n",
    "\n",
    "6. **Inference:** Once we are satisfied with the model's performance, we can use it to predict the x, y coordinates and event type of the tennis ball in new video frames.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da08527a-5cd2-4d12-a22f-cd8641bada8f",
   "metadata": {
    "id": "da08527a-5cd2-4d12-a22f-cd8641bada8f",
    "noteable": {
     "cell_type": "markdown"
    },
    "tags": [
     "show_line_numbers"
    ]
   },
   "source": [
    "## Step 1: Data Preparation\n",
    "\n",
    "In this step, we will extract the frames from the videos and save them as individual images. The labels for each image (the x, y coordinates of the tennis ball and the event type) will be extracted from the accompanying .csv file.\n",
    "\n",
    "We will use the OpenCV library to read the video files and extract the frames. The pandas library will be used to read the .csv file and extract the labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d477351e-3d49-49a1-ae26-96b9e3a3a084",
   "metadata": {},
   "source": [
    "### ImageNet normalization used\n",
    "However, when using pre-trained models like the Vision Transformer google/vit-base-patch16-224, it's important to match the preprocessing steps that were applied to the data during the model's original training. In this case, the model was trained on the ImageNet dataset, which was normalized using the specific mean and standard deviation values for the RGB channels that I mentioned earlier.\n",
    "\n",
    "So, while normalizing to the range {0,1} is not wrong per se, it might not yield the best results when using this specific pre-trained model. The model might perform better if the input images are normalized in the same way the training data was normalized.\n",
    "\n",
    "Therefore, I would recommend adjusting your normalization step to match the ImageNet normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "107418fc-c51f-42a4-8769-6435bc99b5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcd6fb79-001f-4cd0-806c-4f80fa523831",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dcd6fb79-001f-4cd0-806c-4f80fa523831",
    "noteable": {
     "cell_type": "code"
    },
    "outputId": "b1d321e1-7169-46dc-9e32-0cbd656fa80e",
    "tags": [
     "show_line_numbers"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing games:   0%|                                                                              | 0/10 [00:24<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 61\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m clip_dir \u001b[38;5;129;01min\u001b[39;00m game_dir\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClip*\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# Loop over the image files in each clip directory\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m image_file \u001b[38;5;129;01min\u001b[39;00m clip_dir\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 61\u001b[0m         \u001b[43mprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;66;03m# Read the .csv file in each clip directory\u001b[39;00m\n\u001b[1;32m     64\u001b[0m     csv_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(clip_dir\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabel.csv\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "Cell \u001b[0;32mIn[1], line 31\u001b[0m, in \u001b[0;36mprocess_image\u001b[0;34m(image_file)\u001b[0m\n\u001b[1;32m     28\u001b[0m     image \u001b[38;5;241m=\u001b[39m (image \u001b[38;5;241m-\u001b[39m mean) \u001b[38;5;241m/\u001b[39m std\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# Save the preprocessed image\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m     \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_suffix\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError processing image \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36msave\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/vision_transformer/lib/python3.8/site-packages/numpy/lib/npyio.py:518\u001b[0m, in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    517\u001b[0m         file \u001b[38;5;241m=\u001b[39m file \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 518\u001b[0m     file_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m file_ctx \u001b[38;5;28;01mas\u001b[39;00m fid:\n\u001b[1;32m    521\u001b[0m     arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(arr)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the size to resize the images to\n",
    "image_size = (224, 224)\n",
    "\n",
    "# Define the ImageNet mean and standard deviation\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "# Define the path to the dataset directory\n",
    "dataset_dir = Path('Dataset/Dataset')\n",
    "\n",
    "def process_image(image_file):\n",
    "    try:\n",
    "        # Read the image\n",
    "        image = cv2.imread(str(image_file))\n",
    "\n",
    "        # Resize the image\n",
    "        image = cv2.resize(image, image_size)\n",
    "\n",
    "        # Convert the image to RGB and normalize the pixel values\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) / 255.0\n",
    "        image = (image - mean) / std\n",
    "\n",
    "        # Save the preprocessed image\n",
    "        np.save(image_file.with_suffix('.npy'), image)\n",
    "    except Exception as e:\n",
    "        print(f'Error processing image {image_file}: {e}')\n",
    "\n",
    "def process_labels(csv_file):\n",
    "    try:\n",
    "        labels_df = pd.read_csv(csv_file)\n",
    "\n",
    "        # Adjust the x, y coordinates to match the new image size\n",
    "        original_image_size = cv2.imread(str(next(csv_file.parent.glob('*.jpg')))).shape[:2][::-1]\n",
    "        labels_df['x-coordinate'] *= image_size[0] / original_image_size[0]\n",
    "        labels_df['y-coordinate'] *= image_size[1] / original_image_size[1]\n",
    "\n",
    "        # Save the preprocessed labels for each image individually \n",
    "        for index, row in labels_df.iterrows():\n",
    "            label_data = row[['visibility', 'x-coordinate', 'y-coordinate', 'status']].to_numpy()\n",
    "            np.save(csv_file.parent / f'{row[\"file name\"]}_labels.npy', label_data.astype(np.float32))\n",
    "\n",
    "        # Save the updated dataframe\n",
    "        labels_df.to_csv(csv_file.with_name(f'{csv_file.stem}_updated.csv'), index=False)\n",
    "    except Exception as e:\n",
    "        print(f'Error processing labels {csv_file}: {e}')\n",
    "\n",
    "\n",
    "# Loop over the game directories\n",
    "for game_dir in tqdm(list(dataset_dir.glob('game*')), desc='Processing games'):\n",
    "    # Loop over the clip directories in each game directory\n",
    "    for clip_dir in game_dir.glob('Clip*'):\n",
    "        # Loop over the image files in each clip directory\n",
    "        for image_file in clip_dir.glob('*.jpg'):\n",
    "            process_image(image_file)\n",
    "\n",
    "        # Read the .csv file in each clip directory\n",
    "        csv_files = list(clip_dir.glob('Label.csv'))\n",
    "        if csv_files:\n",
    "            process_labels(csv_files[0])\n",
    "\n",
    "# Now, we have the preprocessed images and labels.\n",
    "# We can proceed to the next step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5ca757",
   "metadata": {
    "id": "fc5ca757"
   },
   "source": [
    "## Step 2: Create PyTorch Datasets and DataLoaders\n",
    "\n",
    "In this step, we will create PyTorch `Dataset` objects for the training and validation sets. A `Dataset` is a PyTorch abstraction that allows us to encapsulate our data and provide a way to access it. We will also create `DataLoader` objects, which allow us to load data in batches during training, shuffle the data, and parallelize the data loading process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1408192e",
   "metadata": {
    "id": "1408192e"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import os\n",
    "import glob\n",
    "from torch import tensor\n",
    "\n",
    "class TennisDataset(Dataset):\n",
    "    def __init__(self, dataframe, root_dir, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "    \n",
    "        row = self.dataframe.iloc[idx]\n",
    "    \n",
    "        # Extract the base file name from the file name and the directories\n",
    "        base_file_name = row[\"file name\"].split('.jpg_labels')[0]\n",
    "        clip_directory = row[\"clip directory\"]\n",
    "        game_directory = row[\"game directory\"]\n",
    "    \n",
    "        # Construct the path to the image and labels\n",
    "        image_path = self.root_dir / game_directory / clip_directory / f'{base_file_name}.npy'\n",
    "        labels_path = self.root_dir / game_directory / clip_directory / f'{base_file_name}.jpg_labels.npy'\n",
    "    \n",
    "        # Rest of the code...\n",
    "        image = np.load(image_path, allow_pickle=True)\n",
    "        labels = np.load(labels_path, allow_pickle=True)\n",
    "\n",
    "        # Convert the visibility and status to numeric values\n",
    "        #visibility_mapping = {'not visible': 0, 'easily identifiable': 1, 'not easily identifiable': 2, 'occluded': 3}\n",
    "        #trajectory_mapping = {'flying': 0, 'hit': 1, 'bounding': 2}\n",
    "        #labels[0] = visibility_mapping[labels[0]]\n",
    "        #labels[3] = trajectory_mapping[labels[3]]\n",
    "\n",
    "        # Convert the labels to a PyTorch tensor\n",
    "        labels = tensor(labels, dtype=torch.float32)\n",
    "    \n",
    "        # Print the shape of the image\n",
    "        #print(f\"Original shape: {image.shape}\")\n",
    "    \n",
    "        # Transpose the image dimensions\n",
    "        image = image.transpose((2, 0, 1))\n",
    "    \n",
    "        # Print the new shape of the image\n",
    "        #print(f\"Transposed shape: {image.shape}\")\n",
    "\n",
    "        # Convert the image and labels to PyTorch tensors\n",
    "        image = tensor(image, dtype=torch.float32)\n",
    "\n",
    "        \n",
    "    \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "    \n",
    "        return image, labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26f4b88-178e-4864-b22d-f3de1d18c823",
   "metadata": {},
   "source": [
    "## Step 3: Split the Dataset into Training and Validation Sets\n",
    "\n",
    "In this step, we will split the dataset into training and validation sets. The training set is used to train the model, while the validation set is used to evaluate the model's performance during training. This helps us to monitor the model for overfitting, which occurs when the model performs well on the training data but poorly on new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc5871b9-795a-4080-b905-05f9b19762e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Define the path to the dataset directory\n",
    "dataset_dir = Path('Dataset/Dataset')\n",
    "\n",
    "# Create a list to store the file names and their parent directories\n",
    "data = []\n",
    "\n",
    "# Loop over the game directories\n",
    "for game_dir in dataset_dir.glob('game*'):\n",
    "    # Loop over the clip directories in each game directory\n",
    "    for clip_dir in game_dir.glob('Clip*'):\n",
    "        # Loop over the image files in each clip directory\n",
    "        for image_file in clip_dir.glob('*.npy'):\n",
    "            # Append the base file name and its parent directories to the list\n",
    "            data.append({'file name': os.path.splitext(image_file.name)[0], 'clip directory': clip_dir.name, 'game directory': game_dir.name})\n",
    "\n",
    "# Create a DataFrame from the list\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Split the DataFrame into training and validation sets\n",
    "train_df, val_df = train_test_split(df, test_size=0.2)\n",
    "\n",
    "# Now you can use train_df and val_df to create your datasets\n",
    "train_dataset = TennisDataset(train_df, root_dir=dataset_dir)\n",
    "val_dataset = TennisDataset(val_df, root_dir=dataset_dir)\n",
    "\n",
    "# And create your DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309d3078-026b-4b5b-8bcd-934db86b9c9f",
   "metadata": {},
   "source": [
    "### check one of the image .npy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54626148-bec9-4dee-8fa3-518512391e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/tennis/tracknet\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d31b94a4-b3b8-4a20-8abe-7abedec8eff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224, 3) float64\n"
     ]
    }
   ],
   "source": [
    "test_path = \"Dataset/Dataset/game1/Clip1/0000.npy\"\n",
    "data = np.load(test_path, allow_pickle=True)\n",
    "print(data.shape, data.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55290b7e",
   "metadata": {
    "id": "55290b7e"
   },
   "source": [
    "### Step 3B, data augmentation (optional)\n",
    "In this code, we define a transforms.Compose object that first converts the image to a PIL Image, then applies a random horizontal flip with a probability of 0.5, and finally converts the image back to a tensor. We then modify the TennisDataset class to accept an optional transform argument and apply this transform to the images in the __getitem__ method. If an image is flipped, we also flip the x-coordinate of the ball.\n",
    "\n",
    "Please note that this is a simple example and might not work perfectly for your specific use case. For example, the RandomHorizontalFlip transform uses a fixed random state, so the same images will always be flipped. If you want truly random flipping, you might need to implement your own flipping transform. Also, this code assumes that the 'x' coordinate is the first element in the label array, and that it is a value between 0 and 1 representing the relative position of the ball in the frame. If your data is different, you would need to adjust the code accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13c05fba",
   "metadata": {
    "id": "13c05fba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom torchvision import transforms\\n\\n# Define the data augmentation\\ndata_transforms = transforms.Compose([\\n    transforms.ToPILImage(),\\n    transforms.RandomHorizontalFlip(p=0.5),\\n    transforms.ToTensor(),\\n])\\n\\nclass TennisDataset(Dataset):\\n    def __init__(self, df, frames_dir, transform=None):\\n        self.df = df\\n        self.frames_dir = frames_dir\\n        self.transform = transform\\n\\n    def __len__(self):\\n        return len(self.df)\\n\\n    def __getitem__(self, idx):\\n        row = self.df.iloc[idx]\\n        image = np.load(os.path.join(self.frames_dir, f'{row.clip_number}_{row.frame_number}.jpg.npy'))\\n        label = row[['x', 'y', 'event_type']].values\\n\\n        if self.transform:\\n            image = self.transform(image)\\n            # If the image was flipped, flip the x-coordinate of the ball\\n            if self.transform.transforms[1].p == 1:\\n                label[0] = 1 - label[0]\\n\\n        return torch.from_numpy(image), torch.from_numpy(label)\\n\\n# Create the datasets with data augmentation\\n#train_dataset = TennisDataset(train_df, frames_dir, transform=data_transforms)\\n#al_dataset = TennisDataset(val_df, frames_dir)\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define the data augmentation\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "class TennisDataset(Dataset):\n",
    "    def __init__(self, df, frames_dir, transform=None):\n",
    "        self.df = df\n",
    "        self.frames_dir = frames_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image = np.load(os.path.join(self.frames_dir, f'{row.clip_number}_{row.frame_number}.jpg.npy'))\n",
    "        label = row[['x', 'y', 'event_type']].values\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            # If the image was flipped, flip the x-coordinate of the ball\n",
    "            if self.transform.transforms[1].p == 1:\n",
    "                label[0] = 1 - label[0]\n",
    "\n",
    "        return torch.from_numpy(image), torch.from_numpy(label)\n",
    "\n",
    "# Create the datasets with data augmentation\n",
    "#train_dataset = TennisDataset(train_df, frames_dir, transform=data_transforms)\n",
    "#al_dataset = TennisDataset(val_df, frames_dir)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936ab9c4-a521-4b47-982e-56798dfff6bb",
   "metadata": {
    "id": "936ab9c4-a521-4b47-982e-56798dfff6bb",
    "noteable": {
     "cell_type": "markdown"
    },
    "tags": [
     "show_line_numbers"
    ]
   },
   "source": [
    "## Step 4: Model Preparation\n",
    "\n",
    "In this step, we will load the pre-trained Vision Transformer model, and modify its final layer to match the number of output classes for our tasks. For the x, y coordinate prediction task, we will add a fully connected layer with 2 output units (for the x and y coordinates). For the event type prediction task, we will add a fully connected layer with 3 output units (for the 3 event types), followed by a softmax activation function.\n",
    "\n",
    "We will use the Hugging Face Transformers library to load the pre-trained Vision Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d53dcf6-e6b3-42b9-ace9-5857175f437b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "360c6656a1764c6ca29b87e816ffab88",
      "a9a7637e309f47629666e6c26d604ead",
      "a0f2458163da4d5e8a0731a2aaa3692b",
      "b2aec71425b14cef82277e72b711d8c5",
      "b62a221f14724aa89b581709f409cc18",
      "41726533a5b14b50aa0195b3880acff7",
      "ae0c08319ef142ab975c8c6e7eeaf78c",
      "b7654161092d4854b7127613a413a178",
      "4b2654ba348c435cad6b4dca149e5494",
      "a2313b72c7a64a2d861dcbd7eb1a998c",
      "0692e30fe97d4cc198cae4e937e96eaf"
     ]
    },
    "id": "1d53dcf6-e6b3-42b9-ace9-5857175f437b",
    "noteable": {
     "cell_type": "code"
    },
    "outputId": "96af5039-3642-4b67-91df-b62b8aaf0c84",
    "tags": [
     "show_line_numbers"
    ]
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import ViTModel, ViTConfig\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load the pre-trained Vision Transformer model\n",
    "config = ViTConfig.from_pretrained('google/vit-base-patch16-224')\n",
    "model = ViTModel(config)\n",
    "\n",
    "# Modify the final layer\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Linear(config.hidden_size, 2),  # For the x, y coordinate prediction task\n",
    "    nn.Linear(config.hidden_size, 3),  # For the event type prediction task\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Now, we have the modified Vision Transformer model.\n",
    "# We can proceed to the next step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992e2635",
   "metadata": {
    "id": "992e2635"
   },
   "source": [
    "## Step 5: Define the Loss Function and Optimizer\n",
    "\n",
    "In this step, we will define the loss function and the optimizer. The loss function measures how well the model's predictions match the actual values. The optimizer is used to update the model's parameters based on the gradients of the loss function with respect to the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13cebfc1",
   "metadata": {
    "id": "13cebfc1"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Now, we have the loss function and optimizer.\n",
    "# We can proceed to the next step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e30f114-d548-474b-8b9e-e1b4eab557ea",
   "metadata": {
    "id": "1e30f114-d548-474b-8b9e-e1b4eab557ea",
    "noteable": {
     "cell_type": "markdown"
    },
    "tags": [
     "show_line_numbers"
    ]
   },
   "source": [
    "## Step 5: Training Loop\n",
    "\n",
    "In this step, we will define a training loop where we feed the preprocessed images to the model, compute the loss for both tasks (using a suitable loss function for each task), and update the model's weights based on the total loss. The total loss will be a weighted sum of the two individual losses, where the weights reflect the importance of each task.\n",
    "\n",
    "We will use the PyTorch library to define the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5b6aea9-bdf9-428c-acaa-61d7e00c57a8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 703
    },
    "id": "c5b6aea9-bdf9-428c-acaa-61d7e00c57a8",
    "noteable": {
     "cell_type": "code"
    },
    "outputId": "a9209614-4665-437b-fe39-f493b3af5fe3",
    "tags": [
     "show_line_numbers"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|                                                                                        | 0/10 [00:00<?, ?it/s]\n",
      "Training:   0%|                                                                                     | 0/992 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs:   0%|                                                                                        | 0/10 [00:10<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPooling(last_hidden_state=tensor([[[ 1.4075e+00,  1.0877e+00,  2.0836e+00,  ..., -4.9522e-01,\n",
      "           1.0107e+00, -1.4469e-01],\n",
      "         [ 1.7730e+00,  7.3344e-01,  1.6785e+00,  ...,  4.3754e-01,\n",
      "           1.0780e+00, -1.3120e+00],\n",
      "         [ 1.6538e+00,  1.8222e+00,  1.3716e+00,  ...,  5.6994e-01,\n",
      "           6.7216e-01, -1.5812e+00],\n",
      "         ...,\n",
      "         [ 7.8348e-01,  2.3157e+00,  1.1174e+00,  ..., -6.1076e-01,\n",
      "          -3.3558e-01, -1.1977e+00],\n",
      "         [ 8.3033e-01,  2.3129e+00,  1.1678e+00,  ..., -6.9030e-01,\n",
      "          -3.5379e-01, -1.1818e+00],\n",
      "         [-2.2939e-01,  8.8154e-01,  1.3024e+00,  ...,  1.4277e-01,\n",
      "           2.5424e-01, -8.7588e-01]],\n",
      "\n",
      "        [[ 1.3515e+00,  9.6144e-01,  2.1472e+00,  ..., -6.8164e-01,\n",
      "           8.3853e-01, -1.3803e-01],\n",
      "         [ 1.7667e+00,  7.7777e-01,  1.7557e+00,  ...,  3.9531e-01,\n",
      "           1.0675e+00, -1.3184e+00],\n",
      "         [ 1.4740e+00,  2.3893e+00,  2.2491e+00,  ...,  4.2060e-01,\n",
      "           1.0265e+00, -1.5493e+00],\n",
      "         ...,\n",
      "         [ 7.5531e-01,  2.1728e+00,  1.1352e+00,  ..., -7.5911e-01,\n",
      "          -4.0745e-01, -1.1917e+00],\n",
      "         [ 7.8641e-01,  2.2141e+00,  1.1082e+00,  ..., -8.1729e-01,\n",
      "          -4.8519e-01, -1.1400e+00],\n",
      "         [-2.3452e-01,  8.0213e-01,  1.2662e+00,  ...,  9.7561e-02,\n",
      "           1.4177e-01, -8.7745e-01]],\n",
      "\n",
      "        [[ 3.8586e-01, -3.7611e-01,  1.2212e+00,  ..., -1.4200e+00,\n",
      "           9.0314e-01,  4.7484e-01],\n",
      "         [-1.3237e-01, -9.4974e-01, -5.3129e-01,  ..., -3.1903e-01,\n",
      "           1.2864e-01,  8.8921e-01],\n",
      "         [-3.1691e-02, -1.0052e+00, -5.5787e-01,  ..., -6.8185e-01,\n",
      "           5.0634e-01,  8.7596e-01],\n",
      "         ...,\n",
      "         [ 5.9988e-01,  7.9034e-01,  5.2947e-01,  ..., -1.7098e+00,\n",
      "          -4.7499e-02, -5.7214e-02],\n",
      "         [ 5.2549e-01,  1.0163e+00,  2.4806e-01,  ..., -1.7150e+00,\n",
      "          -5.3921e-02,  1.0030e-01],\n",
      "         [ 3.9490e-01,  8.9926e-01,  2.6105e-01,  ..., -1.6806e+00,\n",
      "          -1.5790e-02,  1.4663e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.4630e+00,  1.1662e+00,  2.0751e+00,  ..., -4.8296e-01,\n",
      "           8.5893e-01, -1.5953e-01],\n",
      "         [ 1.8145e+00,  7.3506e-01,  1.5398e+00,  ...,  7.3405e-01,\n",
      "           1.0155e+00, -1.3654e+00],\n",
      "         [ 1.1271e+00,  1.7285e+00,  1.8843e+00,  ...,  6.7778e-01,\n",
      "           4.2052e-01, -1.4642e+00],\n",
      "         ...,\n",
      "         [ 8.4874e-01,  2.3795e+00,  1.2374e+00,  ..., -5.3364e-01,\n",
      "          -3.9910e-01, -1.1929e+00],\n",
      "         [ 9.0117e-01,  2.4211e+00,  1.2559e+00,  ..., -6.5173e-01,\n",
      "          -4.4803e-01, -1.1994e+00],\n",
      "         [-1.8010e-01,  1.0535e+00,  1.3439e+00,  ...,  1.3942e-01,\n",
      "           1.7736e-01, -8.6008e-01]],\n",
      "\n",
      "        [[-8.3948e-01, -1.5321e+00,  1.5863e+00,  ...,  2.4904e-01,\n",
      "          -1.0961e+00,  2.1653e-01],\n",
      "         [-2.4010e-01, -2.4259e-01,  1.4192e+00,  ..., -1.6012e-01,\n",
      "          -5.0620e-01, -6.1625e-01],\n",
      "         [ 2.5693e-02,  2.8652e-03,  7.2747e-01,  ..., -4.8450e-01,\n",
      "          -4.9017e-01,  1.4091e-01],\n",
      "         ...,\n",
      "         [-5.4195e-01, -2.4764e+00,  1.6138e+00,  ...,  1.9793e-01,\n",
      "           7.5728e-01, -2.9917e-01],\n",
      "         [-1.1824e+00, -2.1757e+00,  9.8805e-01,  ...,  4.4080e-02,\n",
      "           3.6215e-01,  3.5327e-01],\n",
      "         [-7.5140e-01, -3.1651e+00,  1.0936e+00,  ...,  1.6817e-01,\n",
      "           9.7130e-01,  2.7627e-01]],\n",
      "\n",
      "        [[ 4.0164e-01,  1.7907e+00,  1.9339e+00,  ..., -7.3154e-01,\n",
      "           1.0858e+00, -6.5477e-01],\n",
      "         [-1.2252e-01, -1.8509e-01,  2.5343e-01,  ...,  8.6780e-01,\n",
      "           2.3686e-01, -1.7254e-01],\n",
      "         [-7.9367e-01, -3.1540e-01,  8.8257e-01,  ..., -1.2943e-01,\n",
      "           9.9491e-01,  3.4106e-02],\n",
      "         ...,\n",
      "         [ 1.7229e+00,  2.1307e+00,  2.1376e+00,  ..., -6.8334e-01,\n",
      "          -4.9474e-02, -1.2210e+00],\n",
      "         [ 1.6705e+00,  2.2838e+00,  2.1374e+00,  ..., -6.9727e-01,\n",
      "          -1.1744e-01, -1.2008e+00],\n",
      "         [ 1.7237e+00,  2.1265e+00,  2.0250e+00,  ..., -7.6357e-01,\n",
      "          -1.3331e-01, -1.1458e+00]]], grad_fn=<NativeLayerNormBackward>), pooler_output=tensor([[-0.7080,  0.7782,  0.4632,  ...,  0.1265, -0.2022, -0.5439],\n",
      "        [-0.7099,  0.7880,  0.5093,  ...,  0.1342, -0.1937, -0.5436],\n",
      "        [-0.4683,  0.7500,  0.1708,  ..., -0.1012, -0.0416, -0.7161],\n",
      "        ...,\n",
      "        [-0.7174,  0.7723,  0.4723,  ...,  0.1362, -0.2242, -0.5252],\n",
      "        [ 0.2980,  0.2066,  0.3803,  ...,  0.0875,  0.5324,  0.0552],\n",
      "        [ 0.1257,  0.6983,  0.5794,  ...,  0.4954,  0.1278, -0.7507]],\n",
      "       grad_fn=<TanhBackward>), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BaseModelOutputWithPooling' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(outputs)\n\u001b[0;32m---> 31\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Backward pass and optimize\u001b[39;00m\n\u001b[1;32m     34\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/vision_transformer/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/vision_transformer/lib/python3.8/site-packages/torch/nn/modules/loss.py:528\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 528\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vision_transformer/lib/python3.8/site-packages/torch/nn/functional.py:3079\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3075\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, target):\n\u001b[1;32m   3076\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   3077\u001b[0m         mse_loss, (\u001b[38;5;28minput\u001b[39m, target), \u001b[38;5;28minput\u001b[39m, target, size_average\u001b[38;5;241m=\u001b[39msize_average, reduce\u001b[38;5;241m=\u001b[39mreduce, reduction\u001b[38;5;241m=\u001b[39mreduction\n\u001b[1;32m   3078\u001b[0m     )\n\u001b[0;32m-> 3079\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m()):\n\u001b[1;32m   3080\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   3081\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3082\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis will likely lead to incorrect results due to broadcasting. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3083\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(target\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()),\n\u001b[1;32m   3084\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   3085\u001b[0m     )\n\u001b[1;32m   3086\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BaseModelOutputWithPooling' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the number of epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# Loop over the epochs\n",
    "for epoch in tqdm(range(num_epochs), desc='Epochs'):\n",
    "    # Train\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, labels in tqdm(train_dataloader, desc='Training', leave=False):\n",
    "        # Move the images and labels to the GPU if available\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Process the labels\n",
    "        #labels = process_labels(labels)\n",
    "        if labels is None:\n",
    "            print(f'error: labels is {labels}')\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        if outputs is None:\n",
    "            print(f'error: output is {output} for {labels}')\n",
    "        else:\n",
    "            print(outputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "\n",
    "    train_loss /= len(train_dataloader.dataset)\n",
    "\n",
    "    # Validate\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_dataloader, desc='Validation', leave=False):\n",
    "            # Move the images and labels to the GPU if available\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Process the labels\n",
    "            labels = process_labels(labels)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "\n",
    "    val_loss /= len(val_dataloader.dataset)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02f1027-1676-4771-b9a7-d5942e5dba2d",
   "metadata": {
    "id": "f02f1027-1676-4771-b9a7-d5942e5dba2d",
    "noteable": {
     "cell_type": "markdown"
    },
    "tags": [
     "show_line_numbers"
    ]
   },
   "source": [
    "## Step 6: Evaluation\n",
    "\n",
    "After training the model for a certain number of epochs, we will evaluate its performance on a validation set. We will compute the loss and accuracy for each task, and adjust the model's hyperparameters or the training process as needed to improve its performance.\n",
    "\n",
    "We will use the PyTorch library to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3bca20-4dc5-416b-a85a-b1406c16bcc7",
   "metadata": {
    "id": "db3bca20-4dc5-416b-a85a-b1406c16bcc7",
    "noteable": {
     "cell_type": "code"
    },
    "tags": [
     "show_line_numbers"
    ]
   },
   "outputs": [],
   "source": [
    "# Define the DataLoader for the validation data\n",
    "val_loader = val_dataloader  # This is the DataLoader object containing your validation data\n",
    "\n",
    "# Evaluation loop\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # Do not compute gradients\n",
    "    total_loss = 0\n",
    "    for images, labels in val_loader:\n",
    "        # Move the data to the GPU if available\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Compute the average loss\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "\n",
    "    print(f'Validation Loss: {avg_loss}')\n",
    "\n",
    "# Now, we have evaluated the model.\n",
    "# We can proceed to the next step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df708e4a-762c-4e6f-b25c-ee8aeb2f3d5e",
   "metadata": {
    "id": "df708e4a-762c-4e6f-b25c-ee8aeb2f3d5e",
    "noteable": {
     "cell_type": "markdown"
    },
    "tags": [
     "show_line_numbers"
    ]
   },
   "source": [
    "## Step 7: Inference\n",
    "\n",
    "Once we are satisfied with the model's performance, we can use it to predict the x, y coordinates and event type of the tennis ball in new video frames.\n",
    "\n",
    "We will use the PyTorch library to perform inference with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2555ec-c78d-4719-8c3c-76b8a920b9a1",
   "metadata": {
    "id": "5c2555ec-c78d-4719-8c3c-76b8a920b9a1",
    "noteable": {
     "cell_type": "code"
    },
    "tags": [
     "show_line_numbers"
    ]
   },
   "outputs": [],
   "source": [
    "# Define the DataLoader for the test data\n",
    "test_data = ...  # This should be a PyTorch Dataset object containing your test data\n",
    "test_loader = DataLoader(test_data, batch_size=32)\n",
    "\n",
    "# Inference loop\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # Do not compute gradients\n",
    "    for images in test_loader:\n",
    "        # Move the data to the GPU if available\n",
    "        images = images.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Compute the predictions\n",
    "        predictions = outputs.argmax(dim=1)\n",
    "\n",
    "        # Here, you can do whatever you want with the predictions.\n",
    "        # For example, you can visualize the predictions on the images.\n",
    "\n",
    "# Now, we have performed inference with the model.\n",
    "# This is the end of the process.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "display_mode": "default",
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "noteable": {
   "last_delta_id": "974eaaad-d75a-465f-b260-2a979e36c0b5",
   "last_transaction_id": "50aa7114-aae6-4a77-b18f-eb8960fa539a"
  },
  "nteract": {
   "version": "noteable@2.9.0"
  },
  "selected_hardware_size": "small",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0692e30fe97d4cc198cae4e937e96eaf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "360c6656a1764c6ca29b87e816ffab88": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a9a7637e309f47629666e6c26d604ead",
       "IPY_MODEL_a0f2458163da4d5e8a0731a2aaa3692b",
       "IPY_MODEL_b2aec71425b14cef82277e72b711d8c5"
      ],
      "layout": "IPY_MODEL_b62a221f14724aa89b581709f409cc18"
     }
    },
    "41726533a5b14b50aa0195b3880acff7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b2654ba348c435cad6b4dca149e5494": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a0f2458163da4d5e8a0731a2aaa3692b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b7654161092d4854b7127613a413a178",
      "max": 69665,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4b2654ba348c435cad6b4dca149e5494",
      "value": 69665
     }
    },
    "a2313b72c7a64a2d861dcbd7eb1a998c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a9a7637e309f47629666e6c26d604ead": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_41726533a5b14b50aa0195b3880acff7",
      "placeholder": "",
      "style": "IPY_MODEL_ae0c08319ef142ab975c8c6e7eeaf78c",
      "value": "Downloading ()lve/main/config.json: 100%"
     }
    },
    "ae0c08319ef142ab975c8c6e7eeaf78c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b2aec71425b14cef82277e72b711d8c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a2313b72c7a64a2d861dcbd7eb1a998c",
      "placeholder": "",
      "style": "IPY_MODEL_0692e30fe97d4cc198cae4e937e96eaf",
      "value": " 69.7k/69.7k [00:00&lt;00:00, 1.84MB/s]"
     }
    },
    "b62a221f14724aa89b581709f409cc18": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b7654161092d4854b7127613a413a178": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
